# Hyperparameters-Tuning
Hyperparameters Tuning

Here i have two projects

- param_tuning1:

  I compare models between each other(Ensemble Methods, Gaussian Processes, Linear Models, Navies Bayes, Nearest Neighbor, SVM, Trees and XGBoost)
  After this, i chose model with the best perfomence (test score) and do params tuning with GridSearchCV
  
- param_tuning2:

  I compare less amount of models but with all of them do params tuning with RandomizedSearchCV in loop.
  
  Make several pretty visualization graphs(All images with annotations!):
  
      -Compare perfomance of each model and their scores(f1-score, AUC, Precision, Recall)
![newplot (1)](https://user-images.githubusercontent.com/78692457/220389858-bea7eaab-c44e-4be4-977d-d65467a8973e.png)


      -Fit Time of each model.
![newplot (2)](https://user-images.githubusercontent.com/78692457/220389105-e32d309b-267a-44f9-9cce-f796efa0e900.png)
      
      -AUC ROC
![newplot (3)](https://user-images.githubusercontent.com/78692457/220389233-0e3ce838-3a6b-410e-900e-a285c30e0f54.png)

      -Confusion matrixes
![newplot (4)](https://user-images.githubusercontent.com/78692457/220389273-09b83e01-f20c-4ba5-9bb5-3ed1668dc232.png)
![newplot (5)](https://user-images.githubusercontent.com/78692457/220390224-04c4ac66-0cbd-4062-8655-c4d95a8d0922.png)
![newplot (6)](https://user-images.githubusercontent.com/78692457/220390232-72131149-cd5e-41f1-b759-7945c93d04c4.png)
![newplot (7)](https://user-images.githubusercontent.com/78692457/220390239-d3e460bd-567c-4c95-aaf8-b81ab675f01a.png)
![newplot (8)](https://user-images.githubusercontent.com/78692457/220390248-326395ba-ce55-4d39-b012-23d4788f9409.png)


